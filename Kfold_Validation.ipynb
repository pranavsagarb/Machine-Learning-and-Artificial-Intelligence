{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW2P6nr1n6fv"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "\n",
        "*  Apply K-Fold cross-validation method\n",
        "*  Tune the hyperparameters of MLP Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbclXs06n8-b"
      },
      "source": [
        "### Description:\n",
        "\n",
        "The MNIST dataset contains: \n",
        "\n",
        "1. 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n",
        "2. Each image is Size Normalized and Centered \n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "\n",
        "\n",
        "### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the â€™90s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of the MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.\n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
        "\n",
        "\n",
        "### Challenges\n",
        "\n",
        "Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output.\n",
        "\n",
        "![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n",
        "\n",
        "Hence, all these challenges make this a good problem to solve in Machine Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkLmftP9oCeD"
      },
      "source": [
        "## Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2LBE8FOoGrF"
      },
      "source": [
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzKqwLppPJI"
      },
      "source": [
        "## Importing required packages\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWoLTkutpPJN"
      },
      "source": [
        "Loading the dataset from sklearn package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Zg9n4zpPJO"
      },
      "source": [
        "## Loading MNIST dataset from sklearn\n",
        "digits = datasets.load_digits(n_class=10)\n",
        "## Loding the data and storing in x\n",
        "X = digits.data\n",
        "## Loading the target data and storing it in y\n",
        "y = digits.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfXTtoTyHeki"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKvy1plDpPJS"
      },
      "source": [
        "# Activation Functions\n",
        "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
        "# Solvers (Optimizers)\n",
        "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
        "# Learning Rate\n",
        "lr = [0.0001,0.001,0.01,0.1]\n",
        "# Hidden Layers and number of nodes in each layer\n",
        "h = [(5,2),(3,2),(6,3),(7,2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKanYk0rpPJW"
      },
      "source": [
        "## Applying K-Folds cross-validator\n",
        "kf = KFold(n_splits=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqWhE0sspPJd"
      },
      "source": [
        "# Function to Create MLP classifier object with hyper parameters\n",
        "def mlp(a,s,h,lr):\n",
        "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,learning_rate_init=lr)\n",
        "    return clf  \n",
        "# Function to calculate the accuracy\n",
        "def accuracy(actual,predicted):\n",
        "    return accuracy_score(actual,predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYeKG-copPJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8df2566-d44c-4959-cc92-6e7f37e536a3"
      },
      "source": [
        "test_accuracy = []\n",
        "train_accuracy = []\n",
        "for i in range(10):\n",
        "    k1 = np.random.randint(0,len(a))\n",
        "    k2 = np.random.randint(0,len(s))\n",
        "    k3 = np.random.randint(0,len(lr))\n",
        "    k4 = np.random.randint(0,len(h))\n",
        "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
        "     #calling the mlp function with random hyper paramters\n",
        "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
        "    tempTrain = 0\n",
        "    tempTest = 0\n",
        "    for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(X)):\n",
        "        ## Splitting the data into train and test\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        Y_train, Y_test  = y[train_index], y[test_index]\n",
        "        ##fit the data into the model\n",
        "        clf.fit(X_train,Y_train)\n",
        "        ##predicting the values on the fitted model using train data\n",
        "        predTrain = clf.predict((X_train))\n",
        "        #adding the accuracy\n",
        "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
        "        ##predict the values on the fitted model using test data\n",
        "        predTest = clf.predict((X_test))\n",
        "        #adding the accuracy\n",
        "        tempTest = tempTest + accuracy(Y_test,predTest)\n",
        "    ##Calculating the train accuracy\n",
        "    print(f'Number of folds is{nbrOfFolds+1}')\n",
        "    train_accuracy.append(tempTrain*1.0/(nbrOfFolds+1))\n",
        "    ##Calculating the test accuracy\n",
        "    test_accuracy.append(tempTest*1.0/(nbrOfFolds+1))\n",
        "    print(\"(train,test) accuracy = \",tempTrain*1.0/(nbrOfFolds+1), tempTest*1.0/(nbrOfFolds+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  adam \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.6436286042838355 0.5515887156644396\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.5427858974443703 0.508001732244494\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (7, 2)\n",
            "Number of folds is4\n",
            "(train,test) accuracy =  0.10257848521497381 0.09794357832219747\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  adam \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (3, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.1990330749285697 0.19032665181885672\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  adam \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.4529706909959268 0.410149715416976\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  adam \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (6, 3)\n",
            "Number of folds is4\n",
            "(train,test) accuracy =  0.3392253694879709 0.33291017074981444\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.6182562524920749 0.5859675822816135\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  adam \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.6661098737936154 0.5764922048997773\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is4\n",
            "(train,test) accuracy =  0.45799229081440457 0.4290212818609256\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  adam \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (5, 2)\n",
            "Number of folds is4\n",
            "(train,test) accuracy =  0.7462318725643753 0.6600197970799307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6baqLI7VpPJr"
      },
      "source": [
        "#### Plotting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_CMEv2apPJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cf87a23d-9924-4bde-f8c3-acfef9a5dfa6"
      },
      "source": [
        "##Plotting the data\n",
        "xx = np.array(range(1,11))\n",
        "plt.bar(xx,train_accuracy,width=0.2)\n",
        "plt.bar(xx+0.2, test_accuracy,width=0.2)\n",
        "plt.legend([\"Train\",\"Test\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "677BLEdKQ_Mg"
      },
      "source": [
        "# Activation Functions\n",
        "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
        "# Solvers (Optimizers)\n",
        "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
        "# Learning Rate\n",
        "lr = [0.0001,0.001,0.01,0.1]\n",
        "# Hidden Layers and number of nodes in each layer\n",
        "h = [(5,2),(3,2),(6,3),(7,2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEZqLg8G94zI"
      },
      "source": [
        "## Applying K-Folds cross-validator\n",
        "kf = KFold(n_splits=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNlrB8J_-HTX"
      },
      "source": [
        "# Function to Create MLP classifier object with hyper parameters\n",
        "def mlp(a,s,h,lr):\n",
        "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,learning_rate_init=lr)\n",
        "    return clf  \n",
        "# Function to calculate the accuracy\n",
        "def accuracy(actual,predicted):\n",
        "    return accuracy_score(actual,predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wAmoMOA-IzO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a85c7e1-aa16-4aab-a930-b08b32b77950"
      },
      "source": [
        "test_accuracy = []\n",
        "train_accuracy = []\n",
        "for i in range(10):\n",
        "    k1 = np.random.randint(0,len(a))\n",
        "    k2 = np.random.randint(0,len(s))\n",
        "    k3 = np.random.randint(0,len(lr))\n",
        "    k4 = np.random.randint(0,len(h))\n",
        "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
        "     #calling the mlp function with random hyper paramters\n",
        "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
        "    tempTrain = 0\n",
        "    tempTest = 0\n",
        "    for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(X)):\n",
        "        ## Splitting the data into train and test\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        Y_train, Y_test  = y[train_index], y[test_index]\n",
        "        ##fit the data into the model\n",
        "        clf.fit(X_train,Y_train)\n",
        "        ##predicting the values on the fitted model using train data\n",
        "        predTrain = clf.predict((X_train))\n",
        "        #adding the accuracy\n",
        "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
        "        ##predict the values on the fitted model using test data\n",
        "        predTest = clf.predict((X_test))\n",
        "        #adding the accuracy\n",
        "        tempTest = tempTest + accuracy(Y_test,predTest)\n",
        "    ##Calculating the train accuracy\n",
        "    print(f'Number of folds is{nbrOfFolds+1}')\n",
        "    train_accuracy.append(tempTrain*1.0/(nbrOfFolds+1))\n",
        "    ##Calculating the test accuracy\n",
        "    test_accuracy.append(tempTest*1.0/(nbrOfFolds+1))\n",
        "    print(\"(train,test) accuracy = \",tempTrain*1.0/(nbrOfFolds+1), tempTest*1.0/(nbrOfFolds+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.5734557595993323 0.5203116304952699\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.7234279354479689 0.6310517529215359\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.46271563717306624 0.38786867000556485\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (3, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.6001669449081803 0.5175292153589316\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  adam \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (3, 2)\n",
            "Number of folds is3\n",
            "(train,test) accuracy =  0.12993878686700056 0.13633834168057876\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (3, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.3152476349471341 0.2938230383973289\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.4872008903728436 0.44518642181413465\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_base.py:92: RuntimeWarning: invalid value encountered in subtract\n",
            "  tmp = X - X.max(axis=1)[:, np.newaxis]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.46188091263216474 0.4045631608235949\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_base.py:92: RuntimeWarning: invalid value encountered in subtract\n",
            "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.09905397885364496 0.09905397885364496\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of folds is3\n",
            "(train,test) accuracy =  0.47495826377295486 0.44908180300500833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoyI4jo5-9V5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6c40d81b-0262-4457-d2cb-5677f398f079"
      },
      "source": [
        "##Plotting the data\n",
        "xx = np.array(range(1,11))\n",
        "plt.bar(xx,train_accuracy,width=0.2)\n",
        "plt.bar(xx+0.2, test_accuracy,width=0.2)\n",
        "plt.legend([\"Train\",\"Test\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATFklEQVR4nO3df5Bd5X3f8ffHK6kiIKNUbHGqlZCG\nqmARx1jegjGeOBNkW5gOykzsGmI8NobudCYyLo7TyNMOZuSZDqQdpx6jiaM6cpkkRiE4bpVYjop/\ndDqtEyNBxA9JVtgSFZaKIhQDntZEaPj2j73Q62VXe7W6e6/27Ps1s6P7POfZe773jvTRuc8557mp\nKiRJc98b+l2AJKk7DHRJaggDXZIawkCXpIYw0CWpIRb0a8fnnXderVq1ql+7l6Q56cEHH3yuqgYn\n29a3QF+1ahV79+7t1+4laU5K8j+n2uaUiyQ1hIEuSQ1hoEtSQ/RtDl2SOvXyyy8zNjbGSy+91O9S\nembx4sUMDQ2xcOHCjn/HQJd0xhsbG2PJkiWsWrWKJP0uZ9ZVFceOHWNsbIzVq1d3/HtOuUg64730\n0kssW7ZsXoQ5QBKWLVt2yp9IDHRJc8J8CfNXzeT1GuiS1BDOoUuac1Zt/kZXn+/wHdecdPuxY8e4\n6qqrAHjmmWcYGBhgcHD8Zs0HHniARYsWTbuPG2+8kc2bN3PRRRedfsFTMNBnaKq/UNP9xZA09yxb\ntox9+/YBcPvtt3POOefw6U9/+ifGVBVVxRveMPnEx1e+8pVZr9MpF0maodHRUdauXcuHP/xhLrnk\nEo4cOcLIyAjDw8NccsklbNmy5bWx73rXu9i3bx8nTpxg6dKlbN68mbe+9a1cccUVPPvss12px0CX\npNPwgx/8gFtvvZUDBw6wfPly7rjjDvbu3cvDDz/M/fffz4EDB173Oy+88ALvfve7efjhh7niiivY\nvn17V2rpKNCTbEhyKMloks2TbP+tJPtaP3+V5PmuVCdJZ7gLL7yQ4eHh19r33HMP69atY926dRw8\neHDSQD/rrLO4+uqrAXj729/O4cOHu1LLtHPoSQaArcB7gDFgT5KdVfValVV1a9v4TwBv60p1knSG\nO/vss197/Pjjj/OFL3yBBx54gKVLl3LDDTdMei15+0nUgYEBTpw40ZVaOjlCvwwYraonquo4sAPY\neJLx1wP3dKM4SZpLXnzxRZYsWcIb3/hGjhw5wu7du3u6/06uclkOPNXWHgMun2xgkguA1cB3ptg+\nAowArFy58pQKlaRXnalXk61bt461a9dy8cUXc8EFF3DllVf2dP+pqpMPSD4AbKiqm1vtjwCXV9Wm\nScb+BjBUVZ+YbsfDw8M1l7/gwssWpd45ePAgb37zm/tdRs9N9rqTPFhVw5ON72TK5WlgRVt7qNU3\nmetwukWS+qKTQN8DrEmyOskixkN758RBSS4Gfhr48+6WKEnqxLSBXlUngE3AbuAgcG9V7U+yJcm1\nbUOvA3bUdHM4kqRZ0dGt/1W1C9g1oe+2Ce3bu1eWJOlUeaeoJDWEgS5JDeFqi5LmntvP7fLzvXDS\nzd1YPhdg+/btvP/97+dNb3rT6dU7BQNdkqbRyfK5ndi+fTvr1q0z0CXpTHT33XezdetWjh8/zjvf\n+U7uuusuXnnlFW688Ub27dtHVTEyMsL555/Pvn37+NCHPsRZZ511Skf2nTLQJWmGHnvsMb7+9a/z\nve99jwULFjAyMsKOHTu48MILee6553j00UcBeP7551m6dClf/OIXueuuu7j00ktnpR4DXZJm6Fvf\n+hZ79ux5bfncH//4x6xYsYL3ve99HDp0iFtuuYVrrrmG9773vT2px0CXpBmqKj7+8Y/zuc997nXb\nHnnkEb75zW+ydetWvva1r7Ft27ZZr8fLFiVphtavX8+9997Lc889B4xfDfPkk09y9OhRqooPfvCD\nbNmyhYceegiAJUuW8KMf/WjW6vEIXdLcM81lhr3ylre8hc9+9rOsX7+eV155hYULF/KlL32JgYEB\nbrrpJqqKJNx5550A3Hjjjdx8882zdlJ02uVzZ4vL50rqlMvn/n+nu3yuJGkOMNAlqSEMdElzwnxb\nmXsmr9eTot12sjUmzpATOdJcs3jxYo4dO8ayZctI0u9yZl1VcezYMRYvXnxKv2egSzrjDQ0NMTY2\nxtGjR/tdSs8sXryYoaGhU/odA13SGW/hwoWsXr2632Wc8ZxDl6SGMNAlqSEMdElqiI4CPcmGJIeS\njCbZPMWYf5LkQJL9Sb7a3TIlSdOZ9qRokgFgK/AeYAzYk2RnVR1oG7MG+AxwZVX9MMnfm62CJUmT\n6+QI/TJgtKqeqKrjwA5g44Qx/xTYWlU/BKiqZ7tbpiRpOp1ctrgceKqtPQZcPmHMPwRI8t+BAeD2\nqvqziU+UZAQYAVi5cuVM6tUZYqrFycAFyqR+6dZJ0QXAGuAXgOuBf59k6cRBVbWtqoaravjVb8yW\nJHVHJ4H+NLCirT3U6ms3Buysqper6q+Bv2I84CVJPdJJoO8B1iRZnWQRcB2wc8KY/8j40TlJzmN8\nCuaJLtYpSZrGtIFeVSeATcBu4CBwb1XtT7IlybWtYbuBY0kOAN8Ffr2qjs1W0ZKk1+toLZeq2gXs\nmtB3W9vjAj7V+pEk9YF3ikpSQxjoktQQc3L5XK+BlqTX8whdkhrCQJekhjDQJakhDHRJaggDXZIa\nwkCXpIYw0CWpIQx0SWoIA12SGsJAl6SGMNAlqSEMdElqCANdkhrCQJekhpiTy+ee1O3nTtH/Qm/r\nkKQea16gq//8T1XqC6dcJKkhOgr0JBuSHEoymmTzJNs/luRokn2tn5u7X6ok6WSmnXJJMgBsBd4D\njAF7kuysqgMThv5hVW2ahRolSR3o5Aj9MmC0qp6oquPADmDj7JYlSTpVnQT6cuCptvZYq2+iX07y\nSJL7kqyY7ImSjCTZm2Tv0aNHZ1CuJGkq3Top+ifAqqr6OeB+4O7JBlXVtqoarqrhwcHBLu1akgSd\nBfrTQPsR91Cr7zVVdayq/rbV/DLw9u6UJ0nqVCfXoe8B1iRZzXiQXwf8SvuAJD9TVUdazWuBg12t\nUtKkVm3+xqT9h++4pseV6EwwbaBX1Ykkm4DdwACwvar2J9kC7K2qncAtSa4FTgB/A3xsFmuWJE2i\noztFq2oXsGtC321tjz8DfKa7pUmSToV3ikpSQ7iWiySdhjPpPIZH6JLUEAa6JDWEUy5z1FQf88BL\n1qT5ykCXNGNn0vyxnHKRpMYw0CWpIZxykaTZMNVXMcKsfR2jgS41UR/CRP3nlIskNYSBLkkNYaBL\nUkMY6JLUEAa6JDWEgS5JDWGgS1JDGOiS1BAGuiQ1REeBnmRDkkNJRpNsPsm4X05SSYa7V6IkqRPT\nBnqSAWArcDWwFrg+ydpJxi0BPgl8v9tFSpKm18laLpcBo1X1BECSHcBG4MCEcZ8D7gR+vasVSpp7\nXEumLzqZclkOPNXWHmv1vSbJOmBFVU39NTrj40aS7E2y9+jRo6dcrCRpaqd9UjTJG4DPA7823diq\n2lZVw1U1PDg4eLq7liS16STQnwZWtLWHWn2vWgL8LPBfkhwG3gHs9MSoJPVWJ4G+B1iTZHWSRcB1\nwM5XN1bVC1V1XlWtqqpVwF8A11bV3lmpWJI0qWlPilbViSSbgN3AALC9qvYn2QLsraqdJ38G9dxU\nJ6Q8GSU1WkffWFRVu4BdE/pum2LsL5x+WZKkU+WdopLUEAa6JDWEgS5JDWGgS1JDGOiS1BAGuiQ1\nhIEuSQ1hoEtSQxjoktQQBrokNYSBLkkNYaBLUkMY6JLUEAa6JDWEgS5JDWGgS1JDGOiS1BAGuiQ1\nhIEuSQ3R0XeKSnq9VZu/MeW2w3dc08NKpHEdHaEn2ZDkUJLRJJsn2f7PkjyaZF+S/5ZkbfdLlSSd\nzLSBnmQA2ApcDawFrp8ksL9aVW+pqkuB3wQ+3/VKJUkn1ckR+mXAaFU9UVXHgR3AxvYBVfViW/Ns\noLpXoiSpE53MoS8HnmprjwGXTxyU5FeBTwGLgF+c7ImSjAAjACtXrjzVWiVJJ9G1q1yqamtVXQj8\nBvCvphizraqGq2p4cHCwW7uWJNHZEfrTwIq29lCrbyo7gN8+naKkOe/2c6fof6G3dWhe6eQIfQ+w\nJsnqJIuA64Cd7QOSrGlrXgM83r0SJUmdmPYIvapOJNkE7AYGgO1VtT/JFmBvVe0ENiVZD7wM/BD4\n6GwWLUl6vY5uLKqqXcCuCX23tT3+ZJfrkiSdIm/9l6SGMNAlqSEMdElqCANdkhrCQJekhjDQJakh\nDHRJaggDXZIawkCXpIYw0CWpIQx0SWoIA12SGsJAl6SGMNAlqSEMdElqCANdkhrCQJekhjDQJakh\nDHRJaoiOAj3JhiSHkowm2TzJ9k8lOZDkkSTfTnJB90uVJJ3MtIGeZADYClwNrAWuT7J2wrC/BIar\n6ueA+4Df7HahkqST6+QI/TJgtKqeqKrjwA5gY/uAqvpuVf3fVvMvgKHulilJmk4ngb4ceKqtPdbq\nm8pNwDdPpyhJ0qlb0M0nS3IDMAy8e4rtI8AIwMqVK7u5a0ma9zo5Qn8aWNHWHmr1/YQk64F/CVxb\nVX872RNV1baqGq6q4cHBwZnUK0maQieBvgdYk2R1kkXAdcDO9gFJ3gb8DuNh/mz3y5QkTWfaQK+q\nE8AmYDdwELi3qvYn2ZLk2tawfwOcA/xRkn1Jdk7xdJKkWdLRHHpV7QJ2Tei7re3x+i7XJUk6Rd4p\nKkkNYaBLUkMY6JLUEAa6JDWEgS5JDWGgS1JDGOiS1BAGuiQ1hIEuSQ1hoEtSQxjoktQQBrokNYSB\nLkkN0dVvLJLOCLefe5JtL/SuDqnHPEKXpIbwCF1z1qrN35i0//DiHhcinSE8QpekhjDQJakhDHRJ\naggDXZIaoqNAT7IhyaEko0k2T7L955M8lOREkg90v0xJ0nSmDfQkA8BW4GpgLXB9krUThj0JfAz4\narcLlCR1ppPLFi8DRqvqCYAkO4CNwIFXB1TV4da2V2ahRklSBzoJ9OXAU23tMeDymewsyQgwArBy\n5cqZPIUkvWaqexEADi/+lck3NPhu4Z6eFK2qbVU1XFXDg4ODvdy1JDVeJ4H+NLCirT3U6pMknUE6\nCfQ9wJokq5MsAq4Dds5uWZKkUzVtoFfVCWATsBs4CNxbVfuTbElyLUCSf5RkDPgg8DtJ9s9m0ZKk\n1+toca6q2gXsmtB3W9vjPYxPxUiS+sQ7RSWpIQx0SWoIA12SGsJAl6SGMNAlqSEMdElqCANdkhrC\nQJekhjDQJakhDHRJaggDXZIawkCXpIYw0CWpIQx0SWoIA12SGsJAl6SGMNAlqSEMdElqCANdkhrC\nQJekhugo0JNsSHIoyWiSzZNs/ztJ/rC1/ftJVnW7UEnSyU0b6EkGgK3A1cBa4PokaycMuwn4YVX9\nA+C3gDu7Xagk6eQ6OUK/DBitqieq6jiwA9g4YcxG4O7W4/uAq5Kke2VKkqaTqjr5gOQDwIaqurnV\n/ghweVVtahvzWGvMWKv9P1pjnpvwXCPASKt5EXCoWy+kz84Dnpt2VLPN9/dgvr9+8D2A3rwHF1TV\n4GQbFszyjn9CVW0DtvVyn72QZG9VDfe7jn6a7+/BfH/94HsA/X8POplyeRpY0dYeavVNOibJAuBc\n4Fg3CpQkdaaTQN8DrEmyOski4Dpg54QxO4GPth5/APhOTTeXI0nqqmmnXKrqRJJNwG5gANheVfuT\nbAH2VtVO4HeB30syCvwN46E/nzRuGmkG5vt7MN9fP/geQJ/fg2lPikqS5gbvFJWkhjDQJakhDPQZ\nSrIiyXeTHEiyP8kn+11TvyQZSPKXSf6037X0Q5KlSe5L8oMkB5Nc0e+aei3Jra1/B48luSfJ4n7X\nNNuSbE/ybOs+nFf7/m6S+5M83vrzp3tZk4E+cyeAX6uqtcA7gF+dZEmE+eKTwMF+F9FHXwD+rKou\nBt7KPHsvkiwHbgGGq+pnGb94Yj5cGPEfgA0T+jYD366qNcC3W+2eMdBnqKqOVNVDrcc/Yvwf8fL+\nVtV7SYaAa4Av97uWfkhyLvDzjF/pRVUdr6rn+1tVXywAzmrdh/JTwP/qcz2zrqr+K+NX9bVrXwbl\nbuCXelmTgd4FrdUl3wZ8v7+V9MW/A/4F8Eq/C+mT1cBR4CutaacvJzm730X1UlU9Dfxb4EngCPBC\nVf3n/lbVN+dX1ZHW42eA83u5cwP9NCU5B/ga8M+r6sV+19NLSf4x8GxVPdjvWvpoAbAO+O2qehvw\nf+jxx+x+a80Tb2T8P7e/D5yd5Ib+VtV/rZsre3pduIF+GpIsZDzM/6Cq/rjf9fTBlcC1SQ4zvgrn\nLyb5/f6W1HNjwFhVvfrp7D7GA34+WQ/8dVUdraqXgT8G3tnnmvrlfyf5GYDWn8/2cucG+gy1lgf+\nXeBgVX2+3/X0Q1V9pqqGqmoV4yfBvlNV8+rIrKqeAZ5KclGr6yrgQB9L6ocngXck+anWv4urmGcn\nhtu0L4PyUeA/9XLnBvrMXQl8hPGj0n2tn/f3uyj1xSeAP0jyCHAp8K/7XE9PtT6d3Ac8BDzKeK40\nfhmAJPcAfw5clGQsyU3AHcB7kjzO+CeXO3pak7f+S1IzeIQuSQ1hoEtSQxjoktQQBrokNYSBLkkN\nYaBLUkMY6JLUEP8PrTkwP810A6UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}