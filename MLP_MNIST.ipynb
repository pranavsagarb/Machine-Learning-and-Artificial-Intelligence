{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPDGLSN8omMi"
      },
      "source": [
        "At the end of the experiment\n",
        "\n",
        "* understand Multi-Layer Perceptron (MLP)\n",
        "* tune the hyper-parameters of a MLP classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNpSnff_nWmW"
      },
      "source": [
        "A hyperparameter is a parameter whose value is set before the learning process begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ppa_jkQ3B0Z"
      },
      "source": [
        "### What is  MLP ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-NXCwYT3LsF"
      },
      "source": [
        "A multilayer perceptron is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes as shown in below image: \n",
        "\n",
        "**Layer1** :   Input Layer\n",
        "\n",
        "**Layer 2** :  Hidden Layer\n",
        "\n",
        "**Layer 3** : Output Layer\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
        "\n",
        "The number of nodes in the input layer is determined by the dimensionality of our data. \n",
        "\n",
        "The number of nodes in the output layer is determined by the number of classes we have.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1OKA4xynWmX"
      },
      "source": [
        "#### Importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_g-vhFCnWmY"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92sr2eUf0Y9l"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RLdLj8GnWmd"
      },
      "source": [
        "Loading the dataset from sklearn package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rusMmbLnWmf"
      },
      "source": [
        "#Load MNIST datset \n",
        "digits = datasets.load_digits(n_class=10)\n",
        "# Create our X and y data\n",
        "data = digits.data\n",
        "target = digits.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seX_QbLrj4XN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "818ec18f-c7b1-4349-c20a-b1bdc238665c"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8srwHyGFnWml"
      },
      "source": [
        "\n",
        "\n",
        "Split  data into training ,testing  sets. This is done easily with SciKit Learnâ€™s train_test_split function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QyTcVXadQV9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2caLjnnWml"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test =  train_test_split(data, target, test_size=0.2, random_state=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUOCABqynWmq"
      },
      "source": [
        "#function to Create MLP classifier object with hyper parameters\n",
        "def mlp(a,s,h,lr):\n",
        "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,max_iter = 5000 ,learning_rate = 'constant',learning_rate_init=lr)\n",
        "    return clf \n",
        "#function to calculate the accuracy\n",
        "def accuracy(actual,predicted):\n",
        "    return accuracy_score(actual,predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0OwqiyQnWmt"
      },
      "source": [
        "**Let us define the hyper parameters of MLP Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RGkGdvSnWmv"
      },
      "source": [
        "# activation: Activation functions are critical in introducing non-linearity in MLP (in absence of this all layers of MLP combine into a single layer)\n",
        "activation = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
        "#solvers: The following are the methods by which your weights get updated.\n",
        "solvers = [\"lbfgs\",\"sgd\",\"adam\"]\n",
        "#learning rate\n",
        "learning_rate = [0.0001,0.001,0.01,0.1]\n",
        "#hidden layers\n",
        "hidden_layers = [(5,2),(3,2),(6,3),(7,2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4olJ-qZq6OBJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDkAAEe60_9-"
      },
      "source": [
        "In the below code cell we are trying to train a MLP classifer with different hyper parameters. Here we choose a random index value based on size of the hyper parameters list which are defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoChIOiunWm1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddb128b7-4568-4f59-b262-689eb7345c8a"
      },
      "source": [
        "test_accuracy = []\n",
        "validation_accuracy = []\n",
        "train_accuracy = []\n",
        "for i in range(10):\n",
        "    k1 = np.random.randint(0,len(activation))\n",
        "    k2 = np.random.randint(0,len(solvers))\n",
        "    k3 = np.random.randint(0,len(learning_rate))\n",
        "    k4 = np.random.randint(0,len(hidden_layers))\n",
        "    print(\"\\nHyper-parameters = \\n activation = \", activation[k1],    \"\\n solver = \", solvers[k2], \"\\n learning_rate_init = \", learning_rate[k3],         \"\\n hidden_layer_sizes = \", hidden_layers[k4])\n",
        "    #calling the mlp function with random hyper paramters\n",
        "    clf = mlp(activation[k1],solvers[k2],hidden_layers[k4],learning_rate[k3])\n",
        "    #Fitting the data into model\n",
        "    clf.fit(X_train,Y_train)\n",
        "    ## Predicting the values on trained model using train data\n",
        "    predTrain = clf.predict((X_train))\n",
        "    #Calculating the train accuracy\n",
        "    train_accuracy.append(accuracy(Y_train,predTrain))\n",
        "    # Predicting the values on trained model using test data\n",
        "    predTest = clf.predict((X_test))\n",
        "    #Calculating the test accuracy\n",
        "    test_accuracy.append(accuracy(Y_test,predTest))\n",
        "  \n",
        "    print(\"(train,  test) accuracy = \",accuracy(Y_train,predTrain),  accuracy(Y_test,predTest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  adam \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (3, 2)\n",
            "(train,  test) accuracy =  0.791231732776618 0.7694444444444445\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(train,  test) accuracy =  0.8524704244954767 0.7916666666666666\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (6, 3)\n",
            "(train,  test) accuracy =  0.7160751565762005 0.7027777777777777\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_base.py:92: RuntimeWarning: invalid value encountered in subtract\n",
            "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(train,  test) accuracy =  0.09394572025052192 0.11944444444444445\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  adam \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (5, 2)\n",
            "(train,  test) accuracy =  0.6931106471816284 0.6861111111111111\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(train,  test) accuracy =  0.3938761308281141 0.34444444444444444\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(train,  test) accuracy =  0.4022268615170494 0.4\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  adam \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (6, 3)\n",
            "(train,  test) accuracy =  0.9102296450939458 0.8333333333333334\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  adam \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (3, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(train,  test) accuracy =  0.6235212247738344 0.6416666666666667\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  tanh \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (5, 2)\n",
            "(train,  test) accuracy =  0.4001391788448156 0.3638888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGsLoIOnWm4"
      },
      "source": [
        "#### Plotting the accuracies of  train, test  sets; On x-axis in the graph below (once the cell is executed), is the combination of parameters output by the cell above, in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbPs4aACnWm7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9fb593ba-9fd1-4065-828d-401ac635c740"
      },
      "source": [
        "## Plotting the data\n",
        "xx = np.array(range(1,11))\n",
        "plt.bar(xx-0.2,train_accuracy,width=0.2)\n",
        "plt.bar(xx,test_accuracy,width=0.2)\n",
        "plt.legend([\"Train\",\"Test\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}